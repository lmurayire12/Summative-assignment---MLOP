"""
Data Acquisition Module for E-commerce Product Image Dataset

This module handles downloading and organizing product images for engagement prediction.
Extends the previous Adidas USA tabular data work to image-based prediction.
"""

import os
import requests
import pandas as pd
import numpy as np
from pathlib import Path
from PIL import Image
import json
from typing import List, Dict, Tuple
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ProductImageDataAcquisition:
    """
    Handles acquisition and organization of e-commerce product images.
    """
    
    def __init__(self, base_dir: str):
        self.base_dir = Path(base_dir)
        self.data_dir = self.base_dir / "data"
        self.setup_directories()
        
    def setup_directories(self):
        """Create directory structure for image dataset."""
        
        directories = [
            self.data_dir / "train" / "high_engagement",
            self.data_dir / "train" / "low_engagement", 
            self.data_dir / "test" / "high_engagement",
            self.data_dir / "test" / "low_engagement"
        ]
        
        for directory in directories:
            directory.mkdir(parents=True, exist_ok=True)
            
        logger.info(f"Directory structure created at {self.data_dir}")
        
    def generate_synthetic_dataset(self, num_train_per_class: int = 500, 
                                 num_test_per_class: int = 100) -> Dict:
        """
        Generate synthetic product images for demonstration.
        In production, this would connect to your e-commerce image database.
        """
        
        logger.info("Generating synthetic product image dataset...")
        
        # Set random seed for reproducibility
        np.random.seed(42)
        
        dataset_info = {
            "train_high": self._create_synthetic_images(
                self.data_dir / "train" / "high_engagement",
                num_train_per_class, "high", "train"
            ),
            "train_low": self._create_synthetic_images(
                self.data_dir / "train" / "low_engagement", 
                num_train_per_class, "low", "train"
            ),
            "test_high": self._create_synthetic_images(
                self.data_dir / "test" / "high_engagement",
                num_test_per_class, "high", "test"
            ),
            "test_low": self._create_synthetic_images(
                self.data_dir / "test" / "low_engagement",
                num_test_per_class, "low", "test"
            )
        }
        
        # Save dataset metadata
        self._save_dataset_metadata(dataset_info)
        
        logger.info("Synthetic dataset generation completed!")
        return dataset_info
        
    def _create_synthetic_images(self, output_dir: Path, count: int, 
                               engagement_type: str, split: str) -> List[str]:
        """Create synthetic product images with different characteristics."""
        
        created_files = []
        
        for i in range(count):
            # Create different image characteristics based on engagement level
            if engagement_type == "high":
                # High engagement: brighter, more colorful, better composition
                base_brightness = np.random.randint(150, 255)
                color_variance = 50
                structure_size = np.random.randint(80, 140)
            else:
                # Low engagement: duller, less appealing
                base_brightness = np.random.randint(80, 180)
                color_variance = 30
                structure_size = np.random.randint(40, 100)
                
            # Generate image
            img = self._generate_product_image(
                base_brightness, color_variance, structure_size
            )
            
            # Save image
            filename = f"{engagement_type}_{split}_{i:04d}.jpg"
            filepath = output_dir / filename
            img.save(filepath, "JPEG", quality=95)
            created_files.append(str(filepath))
            
        logger.info(f"Created {count} {engagement_type} engagement images in {output_dir}")
        return created_files
        
    def _generate_product_image(self, base_brightness: int, 
                              color_variance: int, structure_size: int) -> Image:
        """Generate a synthetic product image."""
        
        # Create base image
        img_array = np.random.randint(
            max(0, base_brightness - color_variance),
            min(255, base_brightness + color_variance),
            (224, 224, 3), dtype=np.uint8
        )
        
        # Add product-like structure (simulating product in center)
        center_x, center_y = 112, 112
        half_size = structure_size // 2
        
        # Main product area
        x1, x2 = max(0, center_x - half_size), min(224, center_x + half_size)
        y1, y2 = max(0, center_y - half_size), min(224, center_y + half_size)
        
        # Add product structure with different colors
        product_brightness = min(255, base_brightness + 30)
        img_array[y1:y2, x1:x2] = np.random.randint(
            max(0, product_brightness - 20),
            min(255, product_brightness + 20),
            (y2-y1, x2-x1, 3), dtype=np.uint8
        )
        
        # Add some details (simulating logos, textures)
        detail_size = structure_size // 4
        for _ in range(np.random.randint(1, 4)):
            dx = np.random.randint(-structure_size//4, structure_size//4)
            dy = np.random.randint(-structure_size//4, structure_size//4)
            
            det_x1 = max(0, center_x + dx - detail_size//2)
            det_x2 = min(224, center_x + dx + detail_size//2)
            det_y1 = max(0, center_y + dy - detail_size//2)
            det_y2 = min(224, center_y + dy + detail_size//2)
            
            detail_color = np.random.randint(0, 255, 3)
            img_array[det_y1:det_y2, det_x1:det_x2] = detail_color
            
        return Image.fromarray(img_array)
        
    def _save_dataset_metadata(self, dataset_info: Dict):
        """Save dataset metadata for tracking and reproducibility."""
        
        metadata = {
            "dataset_type": "e-commerce_product_engagement",
            "creation_date": pd.Timestamp.now().isoformat(),
            "dataset_info": dataset_info,
            "image_specs": {
                "size": [224, 224],
                "channels": 3,
                "format": "JPEG"
            },
            "classes": {
                0: "low_engagement",
                1: "high_engagement"
            },
            "total_images": sum(len(files) for files in dataset_info.values()),
            "description": "Synthetic product images for engagement prediction training"
        }
        
        metadata_path = self.data_dir / "dataset_metadata.json"
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
            
        logger.info(f"Dataset metadata saved to {metadata_path}")
        
    def download_real_dataset(self, dataset_source: str = "custom"):
        """
        Download real product images from e-commerce APIs or datasets.
        This is where you would integrate with your actual data sources.
        """
        
        logger.info("Real dataset download functionality")
        logger.info("In production, integrate with:")
        logger.info("- E-commerce product APIs")
        logger.info("- Internal product databases")
        logger.info("- Public datasets (Fashion-MNIST, etc.)")
        logger.info("- Web scraping tools (with proper permissions)")
        
        # Placeholder for real implementation
        return {
            "status": "placeholder",
            "message": "Implement real data source integration here"
        }
        
    def validate_dataset(self) -> Dict:
        """Validate the created dataset."""
        
        validation_results = {}
        
        for split in ["train", "test"]:
            for class_name in ["high_engagement", "low_engagement"]:
                class_dir = self.data_dir / split / class_name
                images = list(class_dir.glob("*.jpg"))
                
                validation_results[f"{split}_{class_name}"] = {
                    "count": len(images),
                    "valid_images": self._check_image_validity(images),
                    "path": str(class_dir)
                }
                
        logger.info("Dataset validation completed")
        return validation_results
        
    def _check_image_validity(self, image_paths: List[Path]) -> int:
        """Check if images can be loaded properly."""
        
        valid_count = 0
        for img_path in image_paths:
            try:
                with Image.open(img_path) as img:
                    img.verify()
                valid_count += 1
            except Exception as e:
                logger.warning(f"Invalid image {img_path}: {e}")
                
        return valid_count


def main():
    """Main function to run data acquisition."""
    
    # Initialize data acquisition
    base_dir = Path.cwd()
    data_acquisition = ProductImageDataAcquisition(base_dir)
    
    # Generate synthetic dataset
    dataset_info = data_acquisition.generate_synthetic_dataset(
        num_train_per_class=250,  # Smaller for demo
        num_test_per_class=50
    )
    
    # Validate dataset
    validation_results = data_acquisition.validate_dataset()
    
    print("Dataset Creation Summary:")
    print(f"Total images created: {sum(len(files) for files in dataset_info.values())}")
    print("\nValidation Results:")
    for key, results in validation_results.items():
        print(f"{key}: {results['count']} images ({results['valid_images']} valid)")
        
    print("\nâœ… Data acquisition completed successfully!")
    print("ðŸ’¡ Next step: Run the Jupyter notebook for model training")


if __name__ == "__main__":
    main()
